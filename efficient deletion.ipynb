{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debanuj Nayak\n",
    "# debanuj.nayak@iitgn.ac.in\n",
    "# Indian Institute of Technology Gandhinagar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import pickle\n",
    "from sklearn.metrics import silhouette_score as sil_score,normalized_mutual_info_score as nmi_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kmeans ():\n",
    "    \n",
    "    def __init__(self,org_data,k,T):\n",
    "        self.org_data = org_data\n",
    "        self.dels = []\n",
    "        self.k = k\n",
    "        self.T = T\n",
    "        self.data = None\n",
    "        self.centroids = None\n",
    "        self.clusters = None\n",
    "        self.loss = None\n",
    "        self.assignment = None\n",
    "    \n",
    "    \n",
    "    def fit(self,data):\n",
    "        self.data = data\n",
    "        \n",
    "        self.initialize()\n",
    "        self.lloyds()\n",
    "        #return np.array(fin_centroids),fin_clusters,loss\n",
    "    \n",
    "    \n",
    "    def initialize(self):\n",
    "    \n",
    "        n = self.data.shape[0]\n",
    "        d = self.data.shape[1]\n",
    "        \n",
    "        centroid = random.choice(self.data)\n",
    "        self.centroids = np.array([centroid])\n",
    "\n",
    "        for i in range(self.k-1):\n",
    "            Probs = self.get_probs()\n",
    "            idx = np.random.choice(n,p=Probs)\n",
    "            centroid = self.data[idx]\n",
    "            self.centroids = np.vstack([self.centroids,centroid])\n",
    "\n",
    "        #return init_centroids\n",
    "    \n",
    "    def get_probs(self):\n",
    "    \n",
    "        Probs = np.zeros(len(self.data))\n",
    "        for i in range(len(self.data)):\n",
    "            dis = np.linalg.norm(self.data[i] - self.centroids,axis=1)\n",
    "            Probs[i] = np.min(dis)\n",
    "        Probs = [dist**2 for dist in Probs]\n",
    "        Probs = Probs/sum(Probs)\n",
    "        return Probs\n",
    "    \n",
    "    \n",
    "    def lloyds(self):\n",
    "        T = self.T\n",
    "        \n",
    "        for it in range(self.T):\n",
    "            self.find_clusters()\n",
    "            self.find_centroids()\n",
    "            \n",
    "        #return centroids,clusters,loss\n",
    "    \n",
    "    \n",
    "    def find_clusters(self):\n",
    "        \n",
    "        self.loss = 0\n",
    "        self.clusters = {j:[] for j in range(self.k) }\n",
    "        for i in range(len(self.data)):\n",
    "            dis = np.linalg.norm(self.data[i] - self.centroids,axis=1)\n",
    "            self.loss = self.loss + (np.min(dis))**2\n",
    "            j = np.argmin(dis)\n",
    "            self.clusters[j].append(i)\n",
    "        \n",
    "        #return clusters,loss\n",
    "    \n",
    "    \n",
    "    def find_centroids(self):\n",
    "        \n",
    "        self.centroids = []\n",
    "        for j in self.clusters.keys():\n",
    "            if(len(self.clusters[j]) != 0):\n",
    "                runn_sum = np.zeros(len(self.data[0]))\n",
    "                count = 0\n",
    "                for itm in self.clusters[j]: \n",
    "                    runn_sum  = runn_sum + self.data[itm]\n",
    "                    count +=1\n",
    "                centroid = runn_sum/count\n",
    "                if(len(self.centroids) == 0):\n",
    "                    self.centroids = np.vstack([centroid])\n",
    "                else:\n",
    "                    self.centroids = np.vstack([self.centroids,centroid])\n",
    "            else:\n",
    "                centroid = self.reassign_cluster()\n",
    "                #print(\"type\",type(self.centroids))\n",
    "                if(len(self.centroids) == 0):\n",
    "                    self.centroids = np.vstack([centroid])\n",
    "                else:\n",
    "                    self.centroids = np.vstack([self.centroids,centroid])\n",
    "\n",
    "        self.centroids = np.array(self.centroids)\n",
    "    \n",
    "    def reassign_cluster(self):\n",
    "        \n",
    "        #print(\"reassign_cluster\",centroids.shape)\n",
    "        \n",
    "        self.centroids = np.array(self.centroids)\n",
    "        \n",
    "        if(len(self.centroids) == 0):\n",
    "            centroid = random.choice(self.data)\n",
    "        else:\n",
    "            Probs = self.get_probs()\n",
    "            idx = np.random.choice(len(self.data),p=Probs)\n",
    "            centroid = self.data[idx]\n",
    "\n",
    "        return centroid\n",
    "    \n",
    "    \n",
    "    def delete(self,idx):\n",
    "        self.dels.append(idx)\n",
    "        new_data = np.delete(self.org_data,self.dels,axis=0)\n",
    "        self.fit(new_data)\n",
    "        \n",
    "    \n",
    "    def find_assignment(self):\n",
    "        if(not self.clusters):\n",
    "            pass\n",
    "        else:\n",
    "            arr = np.zeros(len(self.data))\n",
    "            for i in self.clusters.keys():\n",
    "                for j in self.clusters[i]:\n",
    "                    arr[j] = i\n",
    "            self.assignment = arr\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class metadata:\n",
    "    \n",
    "    def __init__(self,centroids,q_centroids,phase,cluster_sizes):\n",
    "        self.centroids = centroids\n",
    "        self.q_centroids = q_centroids\n",
    "        self.phase = phase\n",
    "        self.cluster_sizes = cluster_sizes\n",
    "        \n",
    "class Q_Kmeans(Kmeans):\n",
    "    \n",
    "    def __init__(self,org_data,k,T,gamma,epsilon):\n",
    "        Kmeans.__init__(self,org_data,k,T)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.metadata = None\n",
    "        self.initial_centroids = None\n",
    "        \n",
    "    \n",
    "    def qfit(self,data):\n",
    "        self.data = data\n",
    "        \n",
    "        n = len(self.data)\n",
    "        d = len(self.data[0])\n",
    "        \n",
    "        self.metadata = []\n",
    "        \n",
    "        self.initialize()\n",
    "        self.initial_centroids = copy.deepcopy(self.centroids)\n",
    "        \n",
    "        self.find_clusters()\n",
    "        cluster_sizes = [len(self.clusters[j]) for j in self.clusters.keys()]\n",
    "        m0 = metadata(self.initial_centroids,self.initial_centroids,0,cluster_sizes)\n",
    "        self.metadata.append(m0)\n",
    "        \n",
    "        for tau in range(self.T):\n",
    "            prev_centroids = copy.deepcopy(self.centroids)\n",
    "            self.find_centroids()\n",
    "            \n",
    "            self.imbalance_correction(prev_centroids)\n",
    "            \n",
    "            theta = np.random.random([d])\n",
    "            centroids_hat = self.quantize(self.centroids,theta)\n",
    "            \n",
    "            # stored actual self values here\n",
    "            actual_centroids = copy.deepcopy(self.centroids)\n",
    "            actual_clusters = copy.deepcopy(self.clusters)\n",
    "            actual_loss = self.loss\n",
    "            \n",
    "            # set centroids_hat as self.centroids\n",
    "            self.centroids = centroids_hat\n",
    "            self.find_clusters()\n",
    "            \n",
    "            cluster_sizes = [len(self.clusters[j]) for j in self.clusters.keys()]\n",
    "            m = metadata(actual_centroids,self.centroids,theta,cluster_sizes)\n",
    "            self.metadata.append(m)\n",
    "            \n",
    "            if(self.loss>=actual_loss):\n",
    "                self.centroids = actual_centroids\n",
    "                self.clusters = actual_clusters\n",
    "                self.loss = actual_loss\n",
    "                \n",
    "                break\n",
    "                \n",
    "        \n",
    "    def imbalance_correction(self,prev_centroids):\n",
    "        n = len(self.data)\n",
    "        \n",
    "        for kappa in range(1,self.k+1):\n",
    "            val1 = len(self.clusters[kappa-1])\n",
    "            val2 = self.gamma*n/self.k\n",
    "            if(val1 < val2):\n",
    "                self.centroids[kappa-1] = (val1/val2)*self.centroids[kappa-1] + ((val2 - val1)/val2)*prev_centroids[kappa-1] \n",
    "    \n",
    "    def quantize(self,c,theta):\n",
    "        epsilon = self.epsilon\n",
    "        c = (c/epsilon) + theta - 0.5\n",
    "        c = np.round(c)\n",
    "        c = (c - theta + 0.5)* epsilon\n",
    "        return c\n",
    "    \n",
    "    \n",
    "    def qdelete(self,idx):\n",
    "        self.dels.append(idx)\n",
    "        n = len(self.data)\n",
    "        \n",
    "        p = self.org_data[idx]\n",
    "        \n",
    "        if p in self.initial_centroids:\n",
    "            new_data = np.delete(self.org_data,self.dels,axis=0)\n",
    "            self.qfit(new_data)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            flag = 1\n",
    "            for tau in range(len(self.metadata)-1):\n",
    "    \n",
    "                state = self.metadata[tau+1]\n",
    "                c = state.centroids\n",
    "                theta = state.phase\n",
    "                cluster_sizes = state.cluster_sizes\n",
    "                \n",
    "                #p belongs to which cluster\n",
    "                kappa = self.which_cluster(p,c)\n",
    "                ck = c[kappa]\n",
    "                \n",
    "                #c_prevk\n",
    "                prev_state = self.metadata[tau]\n",
    "                c_prev = prev_state.q_centroids\n",
    "                c_prevk = c_prev[kappa]\n",
    "                \n",
    "                #clustersize\n",
    "                val1 = cluster_sizes[kappa] \n",
    "                \n",
    "                # gamma imbalance check\n",
    "                val2 = self.gamma*n/self.k\n",
    "                if(val1 < val2):\n",
    "                    lag = (val2 - val1)/val2\n",
    "                    ck = (val1/val2)*ck\n",
    "                    ck = ck + lag*c_prevk\n",
    "                 \n",
    "                \n",
    "                #perturbing center by removing point p\n",
    "                perturbed_ck = ck - p/val2\n",
    "                \n",
    "                #quantize\n",
    "                cq = self.quantize(ck,theta)\n",
    "                perturbed_cq = self.quantize(perturbed_ck,theta)\n",
    "                c_prev[kappa] = perturbed_cq\n",
    "                \n",
    "\n",
    "                if not all(cq == perturbed_cq):\n",
    "                    print(\"retrain\")\n",
    "                    flag = 0\n",
    "                    new_data = np.delete(self.org_data,self.dels,axis=0)\n",
    "                    self.qfit(new_data)\n",
    "                    break\n",
    "                else:\n",
    "                    state.cluster_sizes[kappa] = state.cluster_sizes[kappa]-1\n",
    "                    new_state = metadata(c,c_prev,theta,state.cluster_sizes)\n",
    "                    self.metadata[tau+1] = new_state\n",
    "                    self.data = np.delete(self.org_data,self.dels,axis=0)\n",
    "              \n",
    "            if (flag):\n",
    "                print(\"no retrain\")\n",
    "                \n",
    "            print(idx,len(self.data))\n",
    "            print()\n",
    "            print()\n",
    "                    \n",
    "    def which_cluster(self,p,centroids):\n",
    "        dis = np.linalg.norm(p - centroids,axis=1)\n",
    "        kappa = np.argmin(dis)\n",
    "        return kappa\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DC k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class node:\n",
    "    '''\n",
    "    Class for a node of the tree used in the DC tree\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,lvl,parent=None):\n",
    "        self.children = None\n",
    "        self.lvl = lvl\n",
    "        self.dataset = []\n",
    "        self.centroids = []\n",
    "        self.parent = parent\n",
    "        \n",
    "            \n",
    "class tree:\n",
    "    '''\n",
    "    Class for the DC tree\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,w,h):\n",
    "        '''\n",
    "        Sets the tree parameters\n",
    "        w: # of children for each non leaf node\n",
    "        h: height of the DC tree\n",
    "        '''\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "        self.levels = {i:[] for i in range(h)}\n",
    "        self.root = None\n",
    "        self.leaves = []\n",
    "        self.which_leaf = None\n",
    "        \n",
    "        \n",
    "    def initialize(self):\n",
    "        '''\n",
    "        Constructs the tree with tree parameters\n",
    "        '''\n",
    "        self.root = node(0)\n",
    "        self.make_tree(self.root,self.w,self.h)\n",
    "        self.levels[0]=[self.root]\n",
    "        \n",
    "        \n",
    "    def make_tree(self,root,w,h):\n",
    "        if(h>1):\n",
    "            root.children = [ node(root.lvl+1,root) for i in range(w)]\n",
    "            self.levels[root.lvl+1] = self.levels[root.lvl+1] + root.children\n",
    "            if(h>2):\n",
    "                for i in range(w):\n",
    "                    self.make_tree(root.children[i],w,h-1)\n",
    "            elif(h==2):\n",
    "                self.leaves = self.leaves+root.children\n",
    "            \n",
    "        \n",
    "    def lvl_traverse(self):\n",
    "        for i in range(len(self.levels)):\n",
    "            print(i,len(self.levels[i]))\n",
    "            \n",
    "\n",
    "##################################################################################\n",
    "\n",
    "\n",
    "class DC_Kmeans():\n",
    "    \n",
    "    def __init__(self,org_data,k,T,w,h):\n",
    "        self.org_data = org_data\n",
    "        self.dels = []\n",
    "        self.k = k\n",
    "        self.T = T\n",
    "        self.data = None\n",
    "        self.centroids = None\n",
    "        self.clusters = None\n",
    "        self.loss = None\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "        self.tree = None\n",
    "        self.assignment = None\n",
    "        \n",
    "        \n",
    "    def dcfit(self,data):\n",
    "        self.data = data\n",
    "        \n",
    "        n = len(self.data)\n",
    "        d = len(self.data[0])\n",
    "        \n",
    "        self.tree = tree(self.w,self.h+1)\n",
    "        self.tree.initialize()\n",
    "        \n",
    "        self.tree.which_leaf = {i:0 for i in range(n)}\n",
    "        \n",
    "        for i in range(n):\n",
    "            leaf = random.choice(self.tree.leaves)\n",
    "            leaf.dataset.append(np.array(self.data[i]))\n",
    "            self.tree.which_leaf[i]=leaf\n",
    "            \n",
    "        for l in range(self.h,-1,-1):\n",
    "            for node in self.tree.levels[l]:\n",
    "                km = Kmeans(np.array(node.dataset),self.k,self.T)\n",
    "                km.fit(np.array(node.dataset))\n",
    "                c,cl,loss = km.centroids,km.clusters,km.loss\n",
    "                node.centroids = c\n",
    "                if l>0:\n",
    "                    node.parent.dataset.extend(c)\n",
    "                else:\n",
    "                    ## l = 0\n",
    "                    kmf = Kmeans(self.data,self.k,self.T)\n",
    "                    kmf.centroids,kmf.data = c,self.data\n",
    "                    kmf.find_clusters()\n",
    "                    \n",
    "                    self.centroids = c\n",
    "                    self.loss = kmf.loss\n",
    "                    self.clusters = kmf.clusters\n",
    "                    \n",
    "                    \n",
    "    def dcdelete(self,idx):\n",
    "        self.dels.append(idx)\n",
    "        \n",
    "        n = len(self.data)\n",
    "        d = len(self.data[0])\n",
    "        \n",
    "        self.data = np.delete(self.org_data,self.dels,axis=0)\n",
    "        \n",
    "        p = self.org_data[idx]\n",
    "        \n",
    "        node = self.tree.which_leaf[idx]\n",
    "        ind = np.where(node.dataset == p)\n",
    "        node.dataset = np.delete(node.dataset, ind[0][0],axis = 0)\n",
    "\n",
    "        while (node.parent != None):\n",
    "            for centroid in node.centroids :\n",
    "                ind = np.where(node.parent.dataset == centroid)\n",
    "                node.parent.dataset = np.delete(node.parent.dataset,ind[0][0],axis = 0)\n",
    "            \n",
    "            km = Kmeans(np.array(node.dataset),self.k,self.T)\n",
    "            km.fit(np.array(node.dataset))\n",
    "            node.centroids,cl,loss = km.centroids,km.clusters,km.loss\n",
    "            node.parent.dataset = np.vstack([node.parent.dataset,node.centroids])\n",
    "            node = node.parent\n",
    "        \n",
    "        km1 = Kmeans(np.array(node.dataset),self.k,self.T)\n",
    "        km1.fit(np.array(node.dataset))\n",
    "        node.centroids = km1.centroids\n",
    "        \n",
    "        km2 = Kmeans(self.data,self.k,self.T)\n",
    "        km2.centroids,km2.data = node.centroids,self.data\n",
    "        km2.find_clusters()\n",
    "        \n",
    "        self.centroids = km2.centroids\n",
    "        self.clusters = km2.clusters\n",
    "        self.loss = km2.loss\n",
    "\n",
    "        \n",
    "    def find_assignment(self):\n",
    "        if(not self.clusters):\n",
    "            pass\n",
    "        else:\n",
    "            arr = np.zeros(len(self.data))\n",
    "            for i in self.clusters.keys():\n",
    "                for j in self.clusters[i]:\n",
    "                    arr[j] = i\n",
    "            self.assignment = arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all data sets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scaled_data.p - data provided by the authors where all data is scaled to range [0,1] in all dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = open(\"scaled_data.p\",'rb') # open(\"<filename>.p\",'rb')\n",
    "total_data = pickle.load(data_file)\n",
    "\n",
    "mnist = total_data['mnist']\n",
    "botnet = total_data['bot_attack']\n",
    "celltype = total_data['4celltypes_10pca']\n",
    "postures = total_data['postures']\n",
    "covtype = total_data['covtype_multiclass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12009, 10) 4\n"
     ]
    }
   ],
   "source": [
    "def make_data(datatuple):\n",
    "    X = datatuple[0]\n",
    "    y = datatuple[1]\n",
    "    d = X.shape[1]\n",
    "    k = datatuple[2]\n",
    "    print(X.shape,k)\n",
    "    return X,y,d,k\n",
    "\n",
    "X,y,d,k = make_data(celltype) # change data from above \n",
    "n = len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epsilon for Q k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03125\n"
     ]
    }
   ],
   "source": [
    "#epsilon\n",
    "val = - np.log10(n/(k * d**1.5))-3\n",
    "epsilon = 2**np.round(val)\n",
    "print(epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w for DC k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "#w\n",
    "w = int(np.round(n**0.3))\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amortized Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2650146484375\n",
      "188.1123432419876\n"
     ]
    }
   ],
   "source": [
    "# k-means\n",
    "km = Kmeans(X,k,10)\n",
    "\n",
    "st = time.time()\n",
    "km.fit(X)\n",
    "en = time.time()\n",
    "\n",
    "centers,clusters,loss = km.centroids,km.clusters,km.loss\n",
    "\n",
    "print(en-st)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5862061977386475\n",
      "203.91781626205352\n"
     ]
    }
   ],
   "source": [
    "# Q k-means\n",
    "qkm = Q_Kmeans(X,k,10,0.2,epsilon)\n",
    "\n",
    "st = time.time()\n",
    "qkm.qfit(X)\n",
    "en = time.time()\n",
    "\n",
    "centers,clusters,loss = qkm.centroids,qkm.clusters,qkm.loss\n",
    "\n",
    "print(en-st)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.695085048675537\n",
      "322.70859030970854\n"
     ]
    }
   ],
   "source": [
    "# DC k-means\n",
    "dckm = DC_Kmeans(X,k,10,w,1)\n",
    "\n",
    "st = time.time()\n",
    "dckm.dcfit(X)\n",
    "en = time.time()\n",
    "\n",
    "centers,clusters,loss = dckm.centroids,dckm.clusters,dckm.loss\n",
    "\n",
    "print(en-st)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1000 deletions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dels = random.sample(range(len(X)),1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "for d in range(len(dels)):\n",
    "    km.delete(dels[d])\n",
    "en = time.time()\n",
    "\n",
    "print(en-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "for d in range(len(dels)):\n",
    "    qkm.qdelete(dels[d]) \n",
    "en = time.time()\n",
    "\n",
    "print(en-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "for d in range(len(dels)):\n",
    "    dckm.dcdelete(dels[d]) \n",
    "en = time.time()\n",
    "\n",
    "print(en-st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_arr = []\n",
    "nmi_arr = []\n",
    "loss_arr = []\n",
    "\n",
    "for exp in range(5):\n",
    "\n",
    "    km = Kmeans(X,k,10)\n",
    "    km.fit(X)\n",
    "    km.find_assignment()\n",
    "    assignment = km.assignment\n",
    "    \n",
    "    sil = sil_score(X, assignment, metric='euclidean',sample_size=10000)\n",
    "    nmi = nmi_score(assignment,y)\n",
    "    \n",
    "    sil_arr.append(sil)\n",
    "    nmi_arr.append(nmi)\n",
    "    loss_arr.append(km.loss)\n",
    "    print(\" exp {} done\".format(exp))\n",
    "    \n",
    "\n",
    "mean_sil,std_sil = np.mean(sil_arr),np.std(sil_arr)\n",
    "mean_nmi,std_nmi = np.mean(nmi_arr),np.std(nmi_arr)\n",
    "mean_loss,std_loss = np.mean(loss_arr),np.std(loss_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_arr = []\n",
    "nmi_arr = []\n",
    "loss_arr = []\n",
    "\n",
    "for exp in range(5):\n",
    "\n",
    "    qkm = Q_Kmeans(X,k,10,0.2,epsilon)\n",
    "    qkm.qfit(X)\n",
    "    qkm.find_assignment()\n",
    "    assignment = qkm.assignment\n",
    "    \n",
    "    sil = sil_score(X, assignment, metric='euclidean',sample_size=10000)\n",
    "    nmi = nmi_score(assignment,y)\n",
    "    \n",
    "    sil_arr.append(sil)\n",
    "    nmi_arr.append(nmi)\n",
    "    loss_arr.append(qkm.loss)\n",
    "    print(\" exp {} done\".format(exp))\n",
    "    \n",
    "\n",
    "mean_sil,std_sil = np.mean(sil_arr),np.std(sil_arr)\n",
    "mean_nmi,std_nmi = np.mean(nmi_arr),np.std(nmi_arr)\n",
    "mean_loss,std_loss = np.mean(loss_arr),np.std(loss_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_arr = []\n",
    "nmi_arr = []\n",
    "loss_arr = []\n",
    "\n",
    "for exp in range(5):\n",
    "\n",
    "    dckm = DC_Kmeans(X,k,10,w,1)\n",
    "    dckm.dcfit(X)\n",
    "    dckm.find_assignment()\n",
    "    assignment = dckm.assignment\n",
    "    \n",
    "    sil = sil_score(X, assignment, metric='euclidean',sample_size=10000)\n",
    "    nmi = nmi_score(assignment,y)\n",
    "    \n",
    "    sil_arr.append(sil)\n",
    "    nmi_arr.append(nmi)\n",
    "    loss_arr.append(dckm.loss)\n",
    "    print(\" exp {} done\".format(exp))\n",
    "    \n",
    "\n",
    "mean_sil,std_sil = np.mean(sil_arr),np.std(sil_arr)\n",
    "mean_nmi,std_nmi = np.mean(nmi_arr),np.std(nmi_arr)\n",
    "mean_loss,std_loss = np.mean(loss_arr),np.std(loss_arr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
